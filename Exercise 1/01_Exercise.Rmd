---
title: "Exercise 1"
author: "Mads Adrian Simonsen, William Scott Grundeland Olsen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  github_document: default
  html_document:
    df_print: paged
subtitle: TMA4300 Computer Intensive Statistical Models
---

```{r setup, include=FALSE}
library(rmarkdown)  # Dynamic Documents for R
library(knitr)  # A General-Purpose Package for Dynamic Report Generation in R
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, message = FALSE, warning = FALSE,
                      strip.white = TRUE, prompt = FALSE, cache = TRUE,
                      size = "scriptsize", fig.width = 6, fig.height = 4)
```

\newcommand{\E}{\operatorname E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\Exp}{\operatorname{Exponential}}
\newcommand{\Uniform}{\operatorname{Uniform}}
\newcommand{\Betadist}{\operatorname{Beta}}
\newcommand{\Normal}{\operatorname{Normal}}
\newcommand{\SD}{\operatorname{SD}}
\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\T}{\mathsf T}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\e}{\mathrm{e}}


```{r libraries, eval = TRUE, echo = FALSE}
library(tidyverse)
```

# Problem A: Stochastic simulation by the probability integral transform and bivariate techniques

## Subproblem 1.
Let $X \sim \Exp(\lambda)$, with the cumulative density function
$$
  F_X(x) = 1 - e^{-\lambda x},\quad x \geq 0.
$$
Then the random variable $Y := F_X(X)$ has a $\Uniform(0, 1)$ distribution. The probability integral transform becomes

$$
  Y = 1 - e^{-\lambda X} \quad \Leftrightarrow \quad X = -\frac{1}{\lambda}\ln(1 - Y).
$$

It is clear that if $U \sim \Uniform(0, 1)$, then $1 - U \sim \Uniform(0, 1)$, and therefore we may as well say that

\begin{equation}\label{eq:exp_transf}
  X = -\frac{1}{\lambda}\ln(Y).
\end{equation}

Thus, we sample $Y$ from `runif()` and transform it using Equation \eqref{eq:exp_transf}, to sample from the exponential distribution. Figure \ref{fig:exp_hist} shows one million samples drawn from the `generate_from_exp()` function defined in the code chunk below. It also shows the theoretical PDF of the exponential distribution with rate parameter $\lambda = 2$.

```{r generate from exp, fig.cap = "\\label{fig:exp_hist}Normalized histogram of one million samples drawn from the exponential distribution, together with the theoretical PDF, with $\\lambda = 2$."}
set.seed(69)

generate_from_exp <- function(n, rate) {
  Y <- runif(n)   # Generate n Uniform(0, 1) variables
  X <- -(1 / rate) * log(Y)   # Transformation
  return(X)
}

# sample
n <- 1000000  # One million samples
lambda <- 2
exp_samp <- generate_from_exp(n, lambda)


# plot
ggplot() +
  geom_histogram(
    data = as.data.frame(exp_samp),
    mapping = aes(x = exp_samp, y = ..density..),
    binwidth = 0.05,
    boundary = 0
  ) +
  stat_function(
    fun = dexp,
    args = list(rate = lambda),
    aes(col = "Theoretical density")
  ) +
  ylim(0, lambda) +
  xlim(0, 2) +
  ggtitle("Simulating from an exponential distribution") +
  xlab("x") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Theoretically, the mean and variance of $X \sim \Exp(\lambda)$ is $\E(X) = \lambda^{-1}$ and $\Var(X) = \lambda^{-2}$. So for $\lambda = 2$ we would expect $\E(X) = 1/2$ and $\Var(X) = 1/4$. For the simulation we get the mean and variance as calculated in the code block below, showing what we would expect.
```{r, eval = TRUE}
mean(exp_samp)
var(exp_samp)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

### Subsubproblem (a)

We are considering the probability density function
\begin{equation} \label{eq:gx}
  g(x) =
  \begin{cases}
    c x^{\alpha-1} & \text{if } 0 < x < 1,
    \\
    c \mathrm{e}^{-x} & \text{if } x \geq 1,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}
where $c$ is a normalizing constant and $\alpha \in (0, 1)$. If $x \leq 0$ the cumulative distribution function is zero. In the interval $0 < x < 1$ it becomes
$$
  G(x) = \int_{-\infty}^x g(\xi) \,\mathrm{d}\xi = \int_0^x c \xi^{\alpha - 1} \,\mathrm{d}\xi = \frac{c}{\alpha} [\xi^\alpha]_0^x = \frac{c}{\alpha} x^\alpha,
$$
and finally for $x \geq 1$ we have
$$
  G(x) = \int_{-\infty}^x g(\xi) \,\mathrm{d}\xi = \int_0^1 c \xi^{\alpha - 1} \,\mathrm{d}\xi + \int_1^x c \mathrm{e}^{-\xi} \,\mathrm{d}\xi = \left[ \frac{c}{\alpha} \xi^{\alpha} \right]_0^1 - \left[ c \mathrm{e}^{-\xi} \right]_1^x = c \left( \frac{1}{\alpha} - \mathrm{e}^{-x} + \frac{1}{\mathrm{e}} \right),
$$
for $\alpha \in (0, 1)$. That is, the cumulative density function is
$$
  G(x) =
  \begin{cases}
    0 & \text{if } x \leq 0,
    \\
    \frac{c}{\alpha} x^\alpha & \text{if } 0 < x < 1,
    \\
    c \left( \frac{1}{\alpha} - \mathrm{e}^{-x} + \frac{1}{\mathrm{e}} \right) & \text{if } x \geq 1.
  \end{cases}
$$
In this case it is trivial to find $c$. We solve
$$
  1 = \int_\mathbb{R} g(x) \,\mathrm{d}x = \int_0^1 c x^{\alpha - 1} \,\mathrm{d}x + \int_1^\infty c \mathrm{e}^{-x} \,\mathrm{d}x = \frac{c}{\alpha} + \frac{c}{\mathrm{e}},
$$
which gives that
$$
  c = \frac{\alpha \mathrm{e}}{\alpha + \mathrm{e}}.
$$
Writing the cumulative density function using this as $c$ we obtain
$$
  G(x) =
  \begin{cases}
    0 & \text{if } x \leq 0,
    \\
    \frac{\mathrm{e}}{\alpha + \mathrm{e}} x^\alpha & \text{if } 0 < x < 1,
    \\
    1 - \frac{\alpha}{\alpha + \mathrm{e}} \mathrm{e}^{1-x} & \text{if } x \geq 1,
  \end{cases}
$$
for $\alpha \in (0, 1)$.

We may then find the inverse cumulative function. For $x \leq 0$ this is just zero, and for $0 < x < 1$, that is $0 < G(x) < \frac{\mathrm{e}}{\alpha + \mathrm{e}}$, we solve $y = \frac{\mathrm{e}}{\alpha + \mathrm{e}} x^\alpha$ for $x$ giving $G^{-1}(y) = \left( \frac{\alpha + \mathrm{e}}{\mathrm{e}} y \right)^{1/\alpha}$. Similarly for $x \geq 1$, that is $G(x) \geq 1 - \frac{\alpha}{\alpha + \mathrm{e}} = \frac{\mathrm{e}}{\alpha + \mathrm{e}}$, we solve $y = 1 - \frac{\alpha}{\alpha + \mathrm{e}} \mathrm{e}^{1-x}$ for $x$, such that
$$
  G^{-1}(y) =
  \begin{cases}
    \left( \frac{\alpha + \mathrm{e}}{\mathrm{e}} y \right)^{1/\alpha} & \text{if } 0 \leq y < \frac{\mathrm{e}}{\alpha + \mathrm{e}},
    \\
    \ln\left[ \frac{\alpha \mathrm{e}}{(1-y) (\alpha + \mathrm{e})} \right] & \text{if } \frac{\mathrm{e}}{\alpha + \mathrm{e}} \leq y \leq 1,
  \end{cases}
$$
for $\alpha \in (0, 1)$.


### Subsubproblem (b)
Using what we found in (a) we may use the inversion method to sample from $g(x)$ given in Equation \eqref{eq:gx}, as shown in the code block beneath. Figure \ref{fig:g_hist} shows one million samples drawn from `generate_from_gx()` and also the theoretical PDF.

```{r generate from gx, fig.cap = "\\label{fig:g_hist}Normalized histogram of one million samples drawn from $g(x)$ given in Equation \\eqref{eq:gx}, together with the theoretical PDF, with $\\alpha = 0.75$."}
set.seed(69)

generate_from_gx <- function(n, alpha) {
  U <- runif(n)   # Generate n Uniform(0, 1) variables
  bound <- exp(1) / (alpha + exp(1))   # Boundary where G^(-1) changes
  left <- U < bound   # The left of the boundary
  U[left] <- (U[left] / bound)^(1 / alpha)   # Left CDF
  U[!left] <- 1 + log(alpha) - log(1 - U[!left]) - log(alpha + exp(1))   # Right CDF
  return(U)
}

# Sample
n <- 1000000  # One million samples
alpha <- 0.75
gx_samp <- generate_from_gx(n, alpha)   # Generating n samples from g(x)

# The theoretically correct PDF
theo_gx <- function(x, alpha) {
  const <- alpha * exp(1) / (alpha + exp(1))    # Normalizing constant
  func <- rep(0, length(x))   # Vector of zeros of same length as x
  left <- x > 0 & x < 1   # The PDF has one value for 0 < x < 1
  right <- x >= 1         # ... and one value for x >= 1
  func[left] <- const * x[left]^(alpha - 1)   # The value to the left
  func[right] <- const * exp(-x[right])       # The value to the right
  return(func)
}

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(gx_samp),
    mapping = aes(x = gx_samp, y = ..density..),
    binwidth = 0.05,
    boundary = 0
  ) +
  stat_function(
    fun = theo_gx,
    args = list(alpha = alpha),
    aes(col = "Theoretical density")
  ) +
  xlim(0, 5) +
  ggtitle("Simulating from g(x) given in Equation (2)") +
  xlab("x") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Assuming $X \sim g(x)$ we may find the expectation to be
$$
  \E(X) = \int_{\mathbb{R}} x g(x) \,\mathrm{d}x = \int_0^1 c x^\alpha \,\mathrm{d}x + \int_1^\infty c x \mathrm{e}^{-x} \,\mathrm{d}x = \frac{\alpha \e}{(\alpha+1)(\alpha+\e)} + \frac{2 \alpha}{\alpha + \e} \approx 0.768,
$$
when $\alpha = 0.75$. This corresponds approximately to the sample mean shown in the following code block.
```{r, eval = TRUE}
mean(gx_samp)
```
Similarly we may find the theoretical variance to be
$$
  \Var(X) = \E(X^2) - \E(X)^2 \approx 0.705,
$$
also for $\alpha = 0.75$, corresponding approximately to the sample variance given in the code block below.
```{r, eval = TRUE}
var(gx_samp)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

### Subsubproblem (a)
We consider the probability density function
$$
  f(x) = \frac{c \e^{\alpha x}}{(1 + \e^{\alpha x})^2},
$$
for $-\infty < x < \infty$ and $\alpha > 0$. To find the normalizing constant $c$ we make sure that the integral over $\mathbb{R}$ of $f(x)$ is one. That is
$$
  1 = \int_\mathbb{R} f(x) \,\mathrm{d}x = c\int_\mathbb{R} \frac{ \e^{\alpha x}}{(1 + \e^{\alpha x})^2} \,\mathrm{d}x,
$$
and letting $u = 1 + \e^{\alpha x}$, it follows that
$$
  1 = \frac{c}{\alpha} \int_1^\infty \frac{\mathrm{d}u}{u^2} = \frac{c}{\alpha} \left[ -\frac{1}{u} \right]_1^\infty = \frac{c}{\alpha}.
$$
That is, the normalizing constant is $c = \alpha$, for $\alpha > 0$. We may then write the probability density function as
\begin{equation} \label{eq:fx}
  f(x) = \frac{\alpha \e^{\alpha x}}{(1 + \e^{\alpha x})^2},
\end{equation}
for $-\infty < x < \infty$ and $\alpha > 0$.


### Subsubproblem (b)
The cumulative distribution function is given as
$$
  F(x) = \int_{-\infty}^x f(\xi) \,\mathrm{d}\xi = \int_{-\infty}^x \frac{\alpha \e^{\alpha \xi}}{(1 + \e^{\alpha \xi})^2} \,\mathrm{d}\xi.
$$
Again letting $u = 1 + \e^{\alpha \xi}$ it follows that
$$
  F(x) = \int_{1}^{1+\e^{\alpha x}} \frac{\alpha \e^{\alpha \xi}}{u^2} \frac{\mathrm{d}u}{\alpha \e^{\alpha \xi}} = \int_{1}^{1+\e^{\alpha x}} \frac{\mathrm{d}u}{u^2} = \left[ \frac{1}{u} \right]_{1+\e^{\alpha x}}^1 = 1 - \frac{1}{1+\e^{\alpha x}} = \frac{\e^{\alpha x}}{1+\e^{\alpha x}},
$$
which holds for $-\infty < x < \infty$ and $\alpha > 0$.

Solving $y = \e^{\alpha x} / (1 + \e^{\alpha x})$ for $x$ then gives us the inverse cumulative distribution function. Some algebra then gives that
$$
  F^{-1}(y) = \frac{1}{\alpha} \ln\left( \frac{y}{1-y} \right) = \frac{1}{\alpha} \left[ \ln\left( y \right) - \ln\left( 1-y \right) \right],
$$
for $0 < y < 1$ and $\alpha > 0$.


### Subsubproblem (c)

Simulating from $f(x)$ given in Equation \eqref{eq:fx} is shown in the code block below, and the result is shown in Figure \ref{fig:f_hist}. The sampling is done by letting $U \sim \Uniform(0, 1)$ and using inversion sampling.

```{r generate from fx, fig.cap = "\\label{fig:f_hist}Normalized histogram of one million samples drawn from $f(x)$ given in Equation \\eqref{eq:fx}, together with the theoretical PDF, with $\\alpha = 100$."}

generate_from_fx <- function(n, alpha) {
  U <- runif(n)   # Generate n Uniform(0, 1) variables
  X <- 1 / alpha * (log(U) - log(1 - U))    # Using the inverse CDF
  return(X)
}

# Sample
n <- 1000000  # One million samples
alpha <- 100  # Letting alpha be 100
fx_samp <- generate_from_fx(n, alpha)   # Generating n samples from f(x)

# The theoretically correct PDF
theo_fx <- function(x, alpha) {
  return(alpha * exp(alpha * x) / (1 + exp(alpha * x))^2)
}

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(fx_samp),
    mapping = aes(x = fx_samp, y = ..density..),
    binwidth = 0.001,
    boundary = 0
  ) +
  stat_function(
    fun = theo_fx,
    args = list(alpha = alpha),
    aes(col = "Theoretical density")
  ) +
  ggtitle("Simulating from f(x) given in Equation (3)") +
  xlab("x") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Letting $X \sim f(x)$ we may find the expected value to be
$$
  \E(X) = \int_\mathbb{R} xf(x) \,\mathrm{d}x = \int_\mathbb{R} \frac{\alpha x \e^{\alpha x}}{(1 + \e^{\alpha x})^2} \,\mathrm{d}x = 0,
$$
because of symmetry. This is confirmed in the following code block where we see that the sample mean is approximately zero, and can also be seen from the figure. This holds for all $\alpha > 0$.
```{r, eval = TRUE}
mean(fx_samp)
```
We may also find the variance of $X$ to be
$$
  \Var(X) = \E(X^2) = \int_\mathbb{R} \frac{\alpha x^2 \e^{\alpha x}}{(1 + \e^{\alpha x})^2} \,\mathrm{d}x \approx 0.000329,
$$
for $\alpha = 100$. This also corresponds to the sample variance as shown in the code block below.
```{r, eval = TRUE}
var(fx_samp)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.
We wish to simulate from a $\Normal(0, 1)$ distribution using the Box-Muller algoritm. If $X_1 \sim \Uniform(0, 2\pi)$ and $X_2 \sim \Exp(1/2)$, then $Y_1 = \sqrt{X_2} \cos(X_1)$ and $Y_2 = \sqrt{X_2} \sin(X_1)$ are standard normal distributed. We use $Z = \sqrt{X_2} \cos(X_1)$ in the following code block. The result of the simulation can be seen in Figure \ref{fig:Box_Muller}, and it also shows the theoretical probability density function.

```{r generate from Box-Muller, fig.cap = "\\label{fig:Box_Muller}One million samples drawn from a standard normal distribution using the Box-Muller algorithm, together with the theoretical PDF."}

std_normal <- function(n) {
  X1 <- 2 * pi * runif(n)   # n samples from Uniform(0, 2pi)
  X2 <- generate_from_exp(n, 1/2)   # n samples from Exponential(1/2)
  Z <- X2^(1/2) * cos(X1)   # Z ~ Normal(0, 1)
  return(Z)
}

# Sample
n <- 1000000  # One million samples
Box_Muller <- std_normal(n)   # Generating n samples from Normal(0, 1)

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(Box_Muller),
    mapping = aes(x = Box_Muller, y = ..density..),
    binwidth = 0.05,
    boundary = 0
  ) +
  stat_function(
    fun = dnorm,
    aes(col = "Theoretical density")
  ) +
  ggtitle("Simulating from standard normal distribution") +
  xlab("z") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

We know that if $Z \sim \Normal(0, 1)$, then $\E(Z) = 0$ and $\Var(Z) = 1$, and this corresponds to the approximate sample mean and variance shown in the code block below.
```{r, eval = TRUE}
mean(Box_Muller)
var(Box_Muller)
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 5.
We wish to simulate from a $d$-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$. Let $Z \sim \Normal_d(0, I_d)$, where $I_d$ is the identity matrix in $\mathbb{R}^{d \times d}$ and $0$ is the zero-vector in $\mathbb{R}^{d}$. Then
$$
  X = \mu + A Z \sim \Normal_d(\mu, A A^\top),
$$
such that we need to find $A$ such that $\Sigma = A A^\top$, this is done using `chol()` in `R`, and we construct the function in the following code block.

```{r generate from d-variate normal}

d_variate_normal <- function(n, mu, Sigma) {
  d <- length(mu)   # The dimension d is the dimension of mu
  A <- t(chol(Sigma)) # Cholesky decomposition of Sigma. Transpose to get lower triangular
  z <- std_normal(d * n)    # Create vector of d*n independent Normal(0, 1)
  Z <- matrix(z, nrow = d, ncol = n)    # Make z into (d X n) matrix Z
  X <- mu + A %*% Z   # X ~ Normal_d(mu, Sigma) 
  return(X)
}
```
We now test the implementation using one million samples in $\mathbb{R}^3$, with
$$
  \mu =
  \begin{bmatrix}
    1 \\ 7 \\ 2
  \end{bmatrix}
  \quad \text{and} \quad
  \Sigma =
  \begin{bmatrix}
    2 & -1 & 0 
    \\
    -1 & 2 & -1 
    \\
    0 & -1 & 2
  \end{bmatrix}.
$$
We then expect the sample mean and sample covariance matrix to be approximately equal to these. In the following code block we see that this is indeed the case.
```{r, eval = TRUE}
# Sample
n <- 1000000    # One million samples
mu <- c(1, 7, 2)   # Create mu
Sigma <- matrix(c(2, -1, 0, -1, 2, -1, 0, -1, 2), nrow = 3)    # Create Sigma
normal <- d_variate_normal(n, mu, Sigma)    # Sample from d-variate Normal(mu, Sigma)

# Test
rowMeans(normal)  # Finding the mean of the rows of normal, sample mean
cov(t(normal))    # Transpose because we want with respect to the rows
```


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem B: The gamma distribution

## Subproblem 1.

### Subsubproblem (a)

Let $f(x)$ be the target distribution we wish to sample from, and let $g(x)$ be the proposal distribution. For the rejection sampling algorithm, we require that
\begin{equation}\label{eq:rej_samp}
  f(x) \leq c\cdot g(x),\quad\forall x\in\mathbb R,
\end{equation}
for some constant $c>0$. Let $X$ and $U$ be independent samples where $X \sim g(x)$ and $U \sim \Uniform(0, 1)$. Then the acceptance probability is
$$
\begin{split}
  \Pr\left(U\leq\frac{f(X)}{c\cdot g(X)}\right) &= \int_{-\infty}^\infty\int_{0}^{f(x)/(c\ g(x))}f_{X,U}(x, u)\,du\,dx \\
  &= \int_{-\infty}^\infty\int_{0}^{f(x)/(c\ g(x))}g(x)\cdot 1\,du\,dx \\
  &= \int_{-\infty}^\infty\frac{f(x)}{c\ g(x)}g(x)\,dx \\
  &= \frac{1}{c}\int_{-\infty}^\infty f(x)\,dx \\
  &= \frac{1}{c}
\end{split}
$$

We wish to sample from $\operatorname{Gamma}(\alpha, \beta = 1)$, using the proposal distribution $g(x)$ given in \eqref{eq:gx}. We want to choose $c$ such that the acceptance probability is maximized while \eqref{eq:rej_samp} is satisfied. We must check three cases. The trivial case when $x\leq 0$, we have $f(x) = g(x) = 0$ so \eqref{eq:rej_samp} is satisfied for all $c$. When $0<x<1$ we have

$$
\begin{split}
  f(x) &\leq c\,g(x) \\
  \frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} &\leq c\,\frac{1}{\alpha^{-1} + e^{-1}}x^{\alpha - 1} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}e^{-x} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}.
\end{split}
$$

The last case, when $x\geq 1$, we have
$$
\begin{split}
  f(x) &\leq c\,g(x) \\
  \frac{1}{\Gamma(\alpha)}x^{\alpha - 1}e^{-x} &\leq c\,\frac{1}{\alpha^{-1} + e^{-1}}e^{-x} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}x^{\alpha - 1} \\
  c&\geq \left(\frac{1}{\alpha} + \frac{1}{e}\right)\frac{1}{\Gamma(\alpha)}.
\end{split}
$$
That is, we choose $c := (\alpha^{-1} + e^{-1})/\Gamma(\alpha)$, such that the acceptance probability becomes
$$
\Pr\left(U\leq\frac{f(X)}{c\cdot g(X)}\right) = \frac{1}{c} = \frac{\Gamma(\alpha)}{\alpha^{-1} + e^{-1}},\quad \alpha\in(0, 1).
$$


### Subsubproblem (b)
Figure \ref{fig:gamma_rej} shows a sample drawn from the Gamma distribution using the rejection sampling method.

```{r gamma rejection sampling, fig.cap = "\\label{fig:gamma_rej}Normalized histogram of a hundred thousand samples drawn from $\\operatorname{Gamma}(\\alpha = 0.7, \\beta = 1)$ using rejection sampling, together with the theoretical density function."}
set.seed(137)

# Samples from the gamma distribution using rejection sampling with rate parameter = 1
# n:     number of observations
# shape: shape parameter, must be between 0 and 1
sample_from_gamma_rej <- function(n, shape = 0.5) {
  c <- (1 / shape + 1 / exp(1)) / gamma(shape)    # constant that minimizes the envelope
  x <- vector(mode = "numeric", length = n)       # sample vector initialized
  for (i in 1:n) {
    repeat {
      x[i] <- generate_from_gx(1, alpha = shape)  # draw from proposal
      u <- runif(1)                               # draw from U(0, 1)
      f <- dgamma(x[i], shape = shape)            # target value
      g <- theo_gx(x[i], alpha = shape)           # proposal value
      alpha <- (1 / c) * (f / g)                  # acceptance threshold
      if (u <= alpha) {
        break
      }
    }
  }
  return(x)
}

# Sample
n <- 100000  # Hundred thousand observations
alpha <- 0.7
x <- sample_from_gamma_rej(n, shape = alpha)

# Plot
ggplot() +
  geom_histogram(
    mapping = aes(x, after_stat(density)),
    breaks  = seq(0, max(x), by = 0.1)
  ) +
   geom_function(
     mapping = aes(color = "Theoretical density"),
     fun     = dgamma,
     n       = 1001,
     args    = list(shape = alpha),
   ) +
   coord_cartesian(
     xlim = c(0, 4),
     ylim = c(0, 2.5)
   ) +
  theme_minimal()
```

The expectation and variance of $X\sim\operatorname{Gamma}(\alpha = `r alpha`,\beta = 1)$ is $\E[X] = \alpha/\beta = `r alpha`$ and $\Var[x] = \alpha/\beta^2  = `r alpha`$. We compare with the sample mean and sample variance,
```{r gamma sample mean and variance}
list(sample_mean = mean(x), sample_variance = var(x))
```
and see that it corresponds well to the true values.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.

### Subsubroblem (a)
We will now use the ratio-of-uniforms method to simulate from $\operatorname{Gamma}(\alpha,\beta = 1)$. Additionally we have $\alpha>1$ this time. Let us define
\begin{equation}\label{eq:rat-of-uni}
  C_f = \left\{(x_1, x_2) : 0 \leq x_1 \leq \sqrt{f^*\left(\frac{x_2}{x_1}\right)}\right\}, \quad \text{where} \quad f^*(x) = \begin{cases}
           x^{\alpha - 1}e^{-x}, & x > 0,\\
           0, & \text{otherwise},
  \end{cases}
\end{equation}

and

\begin{equation}\label{eq:rat-bounds}
a = \sqrt{\sup_x f^*(x)},\quad b_+ = \sqrt{\sup_{x\geq 0}(x^2 f^*(x))}\quad \text{and}\quad b_- = -\sqrt{\sup_{x\leq 0}(x^2 f^*(x))},
\end{equation}

such that $C_f\subset[0,a]\times[b_-, b_+]$.

First we find $\sup_x f^*(x)$. This must be when $x>0$. We differentiate $f^*(x)$ and setting the expression equal to zero to find the stationary point.
$$
\begin{split}
  0 &= \frac{d}{dx} f^*(x) \\
  &= \frac{d}{dx} x^{\alpha - 1}e^{-x} \\
  &= e^{-x}x^{\alpha - 2}\left((\alpha - 1) - x\right) \\
  \Rightarrow\quad x &= \alpha - 1,\quad \text{where}\quad\alpha > 1.
\end{split}
$$
Since we have only one stationary point, $f^*(x)$ is continuous, $f^*(x) > 0\ \forall x>0$ and $\lim_{x\to0+}f^*(x) = \lim_{x\to\infty}f^*(x) = 0$, then $x = \alpha - 1$ must be the global maximum point. That is
\begin{equation}\label{eq:rat-a}
 a = \sqrt{f^*(\alpha - 1)} = \sqrt{(\alpha - 1)^{\alpha - 1}e^{-(\alpha - 1)}} = \left(\frac{\alpha - 1}{e}\right)^{(\alpha - 1)/2}.
\end{equation}

We now wish to find $b_+$.

$$
\begin{split}
  0 &= \frac{d}{dx} x^2f^*(x) \\
  &= \frac{d}{dx} x^{\alpha + 1}e^{-x} \\
  &= e^{-x}x^{\alpha}\left((\alpha + 1) - x\right) \\
  \Rightarrow\quad x &= \alpha + 1,\quad \text{where}\quad\alpha > 1.
\end{split}
$$
Using the same reasoning as for $a$, we have that $x = \alpha + 1$ is a global maximum point for $x^2f^*(x)$. Then
\begin{equation}\label{eq:rat-b_plus}
  b_+ = \sqrt{(\alpha + 1)^2 f^*(\alpha + 1)} = \sqrt{(\alpha + 1)^{\alpha + 1}e^{-(\alpha + 1)}} = \left(\frac{\alpha + 1}{e}\right)^{(\alpha + 1)/2}.
\end{equation}

Finally, we have that
\begin{equation}\label{eq:rat-b_minus}
  b_- = -\sqrt{\sup_{x\leq 0}(x^2\cdot 0)} = 0.
\end{equation}

### Subsubproblem (b)

To avoid producing `NaNs`, we will implement the ratio-of-uniform method on a log scale.
We get the following log-transformations.
$$
\begin{split}
  X_1\sim \Uniform(0, a)&\Rightarrow \log X_1 = \log a + \log U_1,\quad U_1\sim\Uniform(0,1); \\
  X_2\sim \Uniform(b_-=0, b_+ = b) &\Rightarrow \log X_2 = \log b + \log U_2,\quad U_2\sim\Uniform(0,1); \\
  y = \frac{x_2}{x_1} &\Rightarrow y = \exp\{(\log x_2) - (\log x_1)\}; \\
  0\leq x_1\leq \sqrt{f^*(y)} &\Rightarrow \log x_1 \leq \frac{1}{2}\log f^*(y); \\
  f^*(y) = \begin{cases}y^{\alpha - 1}e^{-y},&y>0, \\ 0, &\text{otherwise},\end{cases} &\Rightarrow \log f^*(y) = \begin{cases}(\alpha - 1)\log y - y,&y>0, \\ -\infty, &\text{otherwise.}\end{cases}
\end{split}
$$

Figure \ref{fig:gamma-rou} shows a sample drawn from the Gamma distribution using the ratio-of-uniform method.

```{r gamma ratio-of-uniform method, fig.cap = "\\label{fig:gamma-rou}Normalized histogram of ten thousand samples drawn from $\\operatorname{Gamma}(\\alpha = 487.9, \\beta = 1)$ using the ratio-of-uniform method, together with the theoretical density function."}
set.seed(434)

# Calculates the logarithmic core of the Gamma(shape = alpha, rate = 1) pdf
# x:      x value(s)
# alpha:  shape parameter
lgamma_core <- function(x, alpha = 2) {
  ifelse(
    test = x <= 0,
    yes  = -Inf,
    no   = (alpha - 1)*log(x) - x
  )
}

# Samples from the Gamma distribution using the ratio-of-uniform method
# n:              number of samples
# shape:          shape parameter
# include_trials: if TRUE, the number of trials will also be returned
sample_from_gamma_rou <- function(n, shape = 2, include_trials = FALSE) {
  log_a <- ((shape - 1) / 2) * (log(shape - 1) - 1)
  log_b <- ((shape + 1) / 2) * (log(shape + 1) - 1)
  trials <- 0
  y <- vector(mode = "numeric", length = n)
  for (i in 1:n) {
    repeat {
      log_x1 <- log_a + log(runif(1))            # draw log x_1, where x_1 ~ U(0, a)
      log_x2 <- log_b + log(runif(1))            # draw log x_2, where x_2 ~ U(0, b)
      y[i] <- exp(log_x2 - log_x1)               # ratio x_2 / x_1
      log_f <- lgamma_core(y[i], alpha = shape)  # log f*(x_2 / x_1)
      if (log_x1 <= 0.5 * log_f) {
        break
      } else {
        trials <- trials + 1
      }
    }
  }
  if (include_trials) {
    return(list(x = y, trials = trials))
  }
  return(y)
}

# Sample
n <- 10000  # Ten thousand observations
alpha <- 487.9
x <- sample_from_gamma_rou(n, shape = alpha)

# Plot
ggplot() +
  geom_histogram(
    mapping = aes(x, after_stat(density)),
    bins = 50
  ) +
  geom_function(
    mapping = aes(color = "Theoretical density"),
    fun     = dgamma,
    n       = 1001,
    args    = list(shape = alpha),
  ) +
  theme_minimal()
```

The expectation and variance of $X\sim\operatorname{Gamma}(\alpha = `r alpha`,\beta = 1)$ is $\E[X] = \alpha/\beta = `r alpha`$ and $\Var[x] = \alpha/\beta^2  = `r alpha`$. We compare with the sample mean and sample variance,
```{r gamma sample mean and variance 2}
list(sample_mean = mean(x), sample_variance = var(x))
```
and see that they are close to the true values.

```{r compare number of trials for each alpha, fig.cap = "\\label{fig:gamma-trials}Number of trials before accepting $N = 1000$ simulations for various shape parameters ($\\alpha$) using the ratio-of-uniform method."}
set.seed(515)

# Sample 1000 observations for each alpha and record number of trials
n <- 1000
m <- 50
alpha <- seq(2, 2000, length.out = m)
trials <- vector(mode = "integer", length = m)  # vector to store number of trials

for (i in 1:m) {
  trials[i] <- sample_from_gamma_rou(n, alpha[i], include_trials = TRUE)$trials
}

# Plot
ggplot(mapping = aes(alpha, trials)) +
  geom_point() +
  theme_minimal()
```

Figure \ref{fig:gamma-trials} strongly suggests that the acceptance probability decreases with increasing $\alpha$. That is, the ratio of the area of the square $a\cdot(b_+ - b_-) = ab$ and the region $C_f$ is increasing when $\alpha$ increases.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

Since $\beta$ is an inverse scale parameter, we can simply draw samples from $\operatorname{Gamma}(\alpha, 1)$ and divide every sample by $\beta$. This can be shown by looking at the mgf of $X/\beta$ where $X\sim \operatorname{Gamma}(\alpha, 1)$.

$$
  M_{X/\beta}(t) = M_X\left(\frac{t}{\beta}\right) = \left(1 - \frac{t}{\beta}\right)^{-\alpha} \sim \operatorname{Gamma}(\alpha,\beta).
$$
When $\alpha = 1$, this is simply the mgf of $\operatorname{Gamma}(1, \beta) \sim \Exp(\beta)$, so we can sample from `generate_from_exp()`. Using this fact together with the rejection sampling method for $0 < \alpha < 1$ and the ratio-of-uniforms method for $\alpha > 1$, we can sample from $\operatorname{Gamma}(\alpha,\beta)$ for any $\alpha>0$ and $\beta>0$.

```{r final gamma sampler}
set.seed(304)

# Samples from the Gamma distribution
# n:      number of samples
# shape:  shape parameter
# rate:   rate parameter
sample_from_gamma <- function(n, shape = 1, rate = 1) {
  if (shape < 1) {
    return(sample_from_gamma_rej(n, shape = shape) / rate)  # rejection sampling
  } else if (shape > 1) {
    return(sample_from_gamma_rou(n, shape = shape) / rate)  # ratio-of-uniform
  }
  generate_from_exp(n, rate = rate)  # shape = 1, draw from the exponential distribution
}

# Sample
n <- 50000  # Fifty thousand observations each
# Create a tibble (dataframe) to store expectation, variance, sample mean and sample
# variance with different combinations of parameters
gamma_tb <- expand_grid(alpha = c(0.5, 1, 30), beta = c(0.3, 14)) %>%
  mutate(expectation = alpha/beta) %>%  # add expectation
  mutate(sample_mean = 0) %>%           # initialize sample mean variable
  mutate(variance = alpha/beta^2) %>%   # add variance
  mutate(sample_variance = 0)           # initialize sample variance variable

for (row in 1:nrow(gamma_tb)) {
  alpha <- gamma_tb[row, ]$alpha  # shape
  beta <- gamma_tb[row, ]$beta    # rate
  x <- sample_from_gamma(n, shape = alpha, rate = beta)  # draw sample
  gamma_tb[row, ]$sample_mean <- mean(x)                 # store sample mean
  gamma_tb[row, ]$sample_variance <- var(x)              # store sample variance
}

gamma_tb
```
The table shows that the expectation and variance corresponds well with the sample mean and sample variance from the samples.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 4.

### Subsubproblem (a)
Let $X$ and $Y$ be independent random variables where $X\sim\operatorname{Gamma}(\alpha, 1)$ and $Y\sim\operatorname{Gamma}(\beta, 1)$. Let
$$
z = g_1(x, y) = \frac{x}{x + y} \quad \text{and}\quad w = g_2(x, y) = x + y
$$
Then $Z = g_1(X, Y)\in(0,1)$ and $W = g_2(X, Y)>0$. This gives us
$$
\begin{split}
  &x = g_1^{-1}(z, w) = zw\quad\text{and}\quad y = g_2^{-1}(z, w) = w(1 - z), \\
  &|\det(J)| = \left|\det\left(\begin{matrix}
    \partial_zg_1^{-1}(z, w) & \partial_wg_1^{-1}(z, w) \\
    \partial_zg_2^{-1}(z, w) & \partial_wg_2^{-1}(z, w)
  \end{matrix}\right)\right| = \left|\det\left(\begin{matrix}
    w & z \\
    -w & 1 - z
  \end{matrix}\right)\right| = |w| = w.
\end{split}
$$
The marginal distribution $f_Z(z)$ is then found as follows.
$$
\begin{split}
  f_Z(z) &= \int_{0}^\infty f_{Z,W}(z, w)\,dw \\
  &=\int_0^\infty f_{X,Y}\left(g_1^{-1}(z, w), g_2^{-1}(z, w)\right)|\det(J)|\,dw \\
  &= \int_0^\infty f_X\left(zw\right)\cdot f_Y\left(w(1 - z)\right)\cdot w\,dw \\
  &= \int_0^\infty \frac{1}{\Gamma(\alpha)}(zw)^{\alpha - 1}e^{-zw}\cdot\frac{1}{\Gamma(\beta)}(w(1 - z))^{\beta - 1}e^{-w(1 - z)}\cdot w\,dw \\
  &= \frac{1}{\Gamma(\alpha)\Gamma(\beta)}z^{\alpha - 1}(1 - z)^{\beta - 1} \Gamma(\alpha + \beta) \int_{0}^\infty \frac{1}{\Gamma(\alpha + \beta)} w^{(\alpha + \beta) - 1}e^{-w}\,dw \\
  &= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}z^{\alpha - 1}(1 - z)^{\beta - 1},\quad z\in(0, 1).\\
\end{split}
$$
That is, 
\begin{equation}\label{eq:beta}
  Z = \frac{X}{X + Y}\sim f_Z(z)\sim \operatorname{Beta}(\alpha, \beta).
\end{equation}


### Subsubproblem (b)

Figure \ref{fig:beta} shows a sample drawn from the beta distribution, with help from `sample_from_gamma()` using \eqref{eq:beta}.
```{r beta sample, fig.cap = "\\label{fig:beta}Normalized histogram of a hundred thousand observations drawn from $\\operatorname{Beta}(\\alpha = 2, \\beta = 5)$."}
set.seed(7)
# Samples from the beta distribution
# n:      number of observations
# shape1: shape parameter
# shape2: shape parameter
sample_from_beta <- function(n, shape1, shape2) {
  x <- sample_from_gamma(n, shape = shape1)
  y <- sample_from_gamma(n, shape = shape2)
  return(x / (x + y))
}

# Sample
n <- 100000  # One hundred thousand observations
alpha <- 2
beta <- 5
x <- sample_from_beta(n, shape1 = alpha, shape2 = beta)

# Plot
ggplot() +
  geom_histogram(
    mapping = aes(x, after_stat(density)),
    bins = 50
  ) +
  geom_function(
    mapping = aes(color = "Theoretical density"),
    fun     = dbeta,
    n       = 1001,
    args    = list(shape1 = alpha, shape2 = beta),
  ) +
  theme_minimal()
```

The expectation and variance of $Z\sim\operatorname{Beta}(\alpha = `r alpha`,\beta = `r beta`)$ is $\E[X] = \alpha/(\alpha + \beta) = `r alpha/(alpha + beta)`$ and $\Var[x] = \alpha\beta/((\alpha + \beta)^2(\alpha + \beta + 1))  = `r alpha * beta / ((alpha + beta)^2 * (alpha + beta + 1))`$. We compare with the sample mean and sample variance,
```{r beta sample mean and variance}
list(sample_mean = mean(x), sample_variance = var(x))
```
and we see that it corresponds well to the theoretical values.

[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------


# Problem C: Monte Carlo integration and variance reduction

## Subproblem 1.

Let $X\sim \operatorname{N}(0,1)$, and $\theta = \Pr(X > 4) \approx `r pnorm(4, lower.tail = FALSE)`$. Let also $h(x) = I(x > 4)$, where $I(\cdot)$ is the indicator function. Then
$$
\E[h(X)] = \int_{-\infty}^\infty h(x)f_X(x)\,dx = \int_{-\infty}^\infty I(x > 4) f_X(x)\,dx = \Pr(X>4) = \theta.
$$

Let $X_1,\ldots X_n\sim\operatorname{N}(0,1)$ be a sample. Then the simple Monte Carlo estimator of $\theta$ is
$$
  \hat\theta_{\mathrm{MC}} = \frac{1}{n}\sum_{i=1}^n h(X_i),
$$
with expectation
$$
  \E\left[\hat\theta_\mathrm{MC}\right] = \frac{1}{n}\sum_{i=1}^n\E\left[h(X_i)\right] = \frac{1}{n}\sum_{i=1}^n\theta = \theta,
$$

and sampling variance
$$
  \widehat{\Var}\left[\hat\theta_\mathrm{MC}\right] = \frac{1}{n^2}\sum_{i=1}^n\widehat{\Var}\left[h(X_i)\right]= \frac{1}{n}\widehat{\Var}[h(X)]=\frac{1}{n(n-1)}\sum_{i=1}^n\left(h(X_i) - \hat\theta_\mathrm{MC}\right)^2.
$$

Then the statistic
$$
  T = \frac{\hat\theta_\mathrm{MC} - \theta}{\sqrt{\widehat{\Var}\left[\hat\theta_\mathrm{MC}\right]}}\sim\mathrm t_{n - 1},
$$
and $t_{\alpha/2,\,n-1} = F_T^{-1}(1 - \alpha/2)$, where $F_T^{-1}(\cdot)$ is the quantile function of the $\mathrm{t}_{n-1}$ distribution.

```{r Monte Carlo integration}
set.seed(321)

theta <- pnorm(4, lower.tail = FALSE)  # true value
n <- 100000                            # number of observations
x <- std_normal(n)                     # draw sample

# Calculates I(x > 4), where I(.) is the indicator function
# x: x-value(s)
h <- function(x) {
  1 * (x > 4)
}

hh <- h(x)                # I(X > 4) vector of ones and zeros
theta_MC <- mean(hh)      # Monte Carlo estimate of Pr(X > 4)
sample_var_MC <- var(hh)  # Sampling variance

t <- qt(0.05/2, df = n - 1, lower.tail = FALSE)  # critical level with 5% significance
ci_MC <- theta_MC + t * sqrt(sample_var_MC / n) * c(-1, 1)  # Confidence Interval

# Result
list(
  theta_MC      = theta_MC,
  sample_var_MC = sample_var_MC,
  confint       = ci_MC,
  error         = abs(theta_MC - theta)
)
```

[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
We will sample from the proposal distribution
$$
  g_X(x) = \begin{cases}cx e^{-\frac{1}{2}x^2}, & x > 4 \\ 0, & \text{otherwise}.\end{cases}
$$
but first we must find the normalizing constant $c$.

$$
\begin{split}
  c &= \left(\int_{4}^\infty x e^{-\frac{1}{2}x^2}\,dx\right)^{-1} 
  = \left(\int_{\frac{1}{2}4^2}^\infty e^{-u}\,du\right)^{-1} 
  = \left(e^{-\frac{1}{2}4^2} - 0\right)^{-1} 
  = e^{\frac{1}{2}4^2}, \\
  \Rightarrow g_X(x) &= \begin{cases}x e^{-\frac{1}{2}(x^2 - 4^2)}, & x > 4, \\ 0, & \text{otherwise}.\end{cases}
\end{split}
$$

We can easily sample from the proposal distribution using inversion sampling. The cdf for $x>4$ is found by integrating.
$$
  G_X(x) = \int_4^x y e^{-\frac{1}{2}(y^2 - 4^2)}\,dy = \int_0^{\frac{1}{2}(x^2 - 4^2)}e^{-u}\,du = 1 - e^{-\frac{1}{2}(x^2 - 4^2)},\quad x> 4,
$$
and $G_X(x) = 0$ for $x \leq 4$. Let $U = G_X(X)\sim \Uniform(0, 1)$. Then we solve for $X$.
$$
\begin{split}
  U &= 1 - e^{-\frac{1}{2}(X^2 - 4^2)} \\
  -\frac{1}{2}(X^2 - 4^2) &= \log(1 - U) \\
  X &= \sqrt{4^2 -2\log(1 - U)}, \quad U\sim \Uniform(0, 1).
\end{split}
$$

Let $X_1,\ldots,X_n$ be a sample drawn from the proposal distribution $g_X(x)$. Then the importance sampling estimator of $\theta$ is given by
$$
  \hat\theta_\mathrm{IS} = \frac{1}{n}\sum_{i=1}^n h(X_i)w(X_i),
$$
where $w(x) = f_X(x)/g_X(x)$, with expectation

$$
\begin{split}
\E\left[\hat\theta_\mathrm{IS}\right] &= \frac{1}{n}\sum_{i=1}^n\int_{0}^\infty h(x_i)w(x_i)g_X(x_i)\,dx_i \\
&= \frac{1}{n}\sum_{i=1}^n\int_{0}^\infty h(x_i)f_X(x_i)\,dx_i \\
&= \frac{1}{n}\sum_{i=1}^n\E\left[h(X_i)\mid X_i\sim \operatorname{N}(0, 1)\right] \\
&= \frac{1}{n}\sum_{i=1}^n\theta \\
&= \theta,
\end{split}
$$

and sampling variance

$$
  \widehat{\Var}\left[\hat\theta_\mathrm{IS}\right] = \frac{1}{n^2}\sum_{i=1}^n\widehat{\Var}\left[h(X_i)w(X_i)\right]= \frac{1}{n}\widehat{\Var}[h(X)w(X)]=\frac{1}{n(n-1)}\sum_{i=1}^n\left(h(X_i)w(X_i) - \hat\theta_\mathrm{IS}\right)^2.
$$
```{r Importance Sampling}
set.seed(321)

# Samples from the proposal distribution
# n: number of observations
sample_from_proposal <- function(n) {
  u <- runif(n)
  sqrt(4^2 - 2 * log(1 - u))
}

# Sample
n <- 100000                   # Hundred thousand observations
x <- sample_from_proposal(n)  # sample

# Computes the weight f(x) / g(x) where f(x) is the target density and g(x) is the
# proposal density.
# x: x-value(s)
w <- function(x) {
  f <- dnorm(x)                              # target density
  g <- ifelse(                               # proposal density
         test = x > 4,
         yes  = x * exp(-0.5 * (x^2 - 16)),
         no   = 0
       )
  return(f / g)
}

hw <- h(x) * w(x)

theta_IS <- mean(hw)      # Importance sampling estimate of Pr(X > 4)
sample_var_IS <- var(hw)  # Sampling variance

t <- qt(0.05/2, df = n - 1, lower.tail = FALSE)  # critical level with 5% significance
ci_IS <- theta_IS + t * sqrt(sample_var_IS / n) * c(-1, 1)  # Confidence Interval

# Result
list(
  theta_IS      = theta_IS,
  sample_var_IS = sample_var_IS,
  confint       = ci_IS,
  error         = abs(theta_IS - theta)
)
```

We see that using this importance sampling method is more precise by far. The number of samples $m$ needed for the simple Monte Carlo estimator to achieve the same precision as the importance sampling approach, we would need

$$
  m = n\frac{\widehat{\Var}[h(X)]}{\widehat{\Var}[h(X)w(X)]} = `r n`\frac{`r sample_var_MC`}{`r sample_var_IS`} = `r n * sample_var_MC / sample_var_IS`,
$$
samples. That is, we need about $10$ million times more samples.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.

### Subsubproblem (a)
We modify `sample_from_proposal()` to return a pair of samples, where one takes $U\sim\Uniform(0,1)$ as argument and the other $1 - U$ as argument.

Let $X_1^{(1)},\ldots X_n^{(1)}$ and $X_1^{(2)},\ldots,X_n^{(2)}$ be a such sample pair drawn from the proposal distribution, with the importance sampling estimators being
$$
  \hat\theta_\mathrm{IS}^{(1)} = \frac{1}{n}\sum_{i=1}^nh(X_i^{(1)})w(X_i^{(1)})\quad\text{and}\quad \hat\theta_\mathrm{IS}^{(2)} = \frac{1}{n}\sum_{i=1}^nh(X_i^{(2)})w(X_i^{(2)}).
$$
The antithetic estimator is then given by

$$
  \hat\theta_\mathrm{AS} = \frac{\hat\theta_\mathrm{IS}^{(1)} + \hat\theta_\mathrm{IS}^{(2)}}{2},
$$
with expectation

$$
  \E\left[\hat\theta_\mathrm{AS}\right] = \frac{1}{2}\left(\E\left[\hat\theta_\mathrm{IS}^{(1)}\right] + \E\left[\hat\theta_\mathrm{IS}^{(2)}\right]\right) = \theta,
$$
and variance

$$
\begin{split}
  \Var\left[\hat\theta_\mathrm{AS}\right]
  &= \frac{1}{4}\left(\Var\left[\hat\theta_\mathrm{IS}^{(1)}\right] + \Var\left[\hat\theta_\mathrm{IS}^{(2)}\right] + 2\Cov\left[\hat\theta_\mathrm{IS}^{(1)}, \hat\theta_\mathrm{IS}^{(2)}\right]\right) \\
  &= \frac{\Var\left[h(X)w(X)\right]}{2n}\left(1 + \Corr\left[h(X^{(1)})w(X^{(1)}), h(X^{(2)})w(X^{(2)})\right]\right).
\end{split}
$$

```{r Proposal function modified}
# Samples from the proposal distribution returning a list consisting of n pairs of
# antithetic variates, with one taking U ~ Uniform(0, 1) as argument, and the other takes
# (1 - U) as argument.
# n: number of observations
sample_from_proposal_mod <- function(n) {
  u <- runif(n)
  list(
    x_1 = sqrt(4^2 - 2 * log(1 - u)),
    x_2 = sqrt(4^2 - 2 * log(u))
  )
}
```


### Subsubproblem (b)

```{r Antithetic sampling}
set.seed(53)

# Sample
n <- 50000  # Fifty thousand observations 
sample_pair <- sample_from_proposal_mod(n)  # n pairs of samples

hw1 <- h(sample_pair$x_1) * w(sample_pair$x_1)  # first of pair 
hw2 <- h(sample_pair$x_2) * w(sample_pair$x_2)  # second of pair
hw_AS <- 0.5 * (hw1 + hw2)  # Antithetic sample

theta_AS <- mean(hw_AS)      # Antithetic sampling estimate of Pr(X > 4)
sample_var_AS <- var(hw_AS)  # Sampling variance

t <- qt(0.05/2, df = n - 1, lower.tail = FALSE)  # critical level with 5% significance
ci_AS <- theta_AS + t * sqrt(sample_var_AS / n) * c(-1, 1)  # Confidence Interval

# Result
list(
  theta_AS      = theta_AS,
  sample_var_AS = sample_var_AS,
  sample_corr   = cor(hw1, hw2),
  confint       = ci_AS,
  error         = abs(theta_AS - theta)
)
```

We see an better estimate of $\theta$, even with half the sample size compared to the importance sampling.

Above we found that
\begin{equation}\label{eq:antith-var}
  \Var\left[\hat\theta_\mathrm{AS}\right]
  = \frac{\Var\left[h(X)w(X)\right]}{2n}\left(1 + \Corr\left[h(X^{(1)})w(X^{(1)}), h(X^{(2)})w(X^{(2)})\right]\right),
\end{equation}
which means that when the correlation term is equal to zero, we would expect about the same variance as in the importance sampling method with half the sample size. The sampling correlation computed above is
$$
  \widehat{\Corr}\left[h(X^{(1)})w(X^{(1)}), h(X^{(2)})w(X^{(2)})\right] = `r cor(hw1, hw2)`,
$$
which explains why we get an even more precise estimate.


[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------

# Problem D: Rejection sampling and importance sampling

## Subproblem 1.
We consider a vector of multinomially distributed counts $\bfy = \begin{bmatrix} y_1 & y_2 & y_3 & y_4 \end{bmatrix}^\top$ and the observed data is $\bfy = \begin{bmatrix} 125 & 18 & 20 & 34 \end{bmatrix}^\top$. The multinomial mass function is given as
$$
  f(\bfy \mid \theta) \propto (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_3},
$$
and assuming a prior that is $\Uniform(0, 1)$ the posterior will be
$$
  f(\theta \mid \bfy) \propto f^*(\theta) := (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_3},
$$
for $\theta \in (0, 1)$. We wish to sample from this using a $\Uniform(0, 1)$ proposal density, that is, $g(\theta \mid \bfy) = 1$, for $\theta \in (0, 1)$. To do a rejection sampling (not weighted rejection sampling), we need to know the normalizing constant of $f(\theta \mid \bfy)$. That is, the constant $K$ such that $f(\theta \mid \bfy) = K f^*(\theta \mid \bfy)$. This can be found as
$$
  \frac 1 K = \int_\mathbb{R} f^*(\theta \mid \bfy) \, \mathrm{d} \theta = \int_0^1 f^*(\theta \mid \bfy) \, \mathrm{d} \theta \approx 2.3577 \cdot 10^{28},
$$
and we find it using the `integrate()`-function in `R` below. To use the rejection sampling we also need that
$$
  \frac{f(\theta \mid \bfy)}{g(\theta \mid \bfy)} = f(\theta \mid \bfy) \leq k,
$$
and a value for $k$ is found in the code block below. This is done by setting $k = \max_\theta f^*(\theta \mid \bfy)$, and for the observed $\bfy$, we have that
$$
  \frac{\mathrm{d} f^*(\theta \mid \bfy)}{\mathrm{d} \theta} = (\theta-1)^{37} \theta^{33} (\theta + 2)^{124} (197 \theta^2 - 15 \theta - 68) = 0.
$$
Because $\theta \in (0, 1)$, the only factor we need to consider is $197 \theta^2 - 15 \theta - 68 = 0$, giving
$$
  \theta_\mathrm{max} = \frac{15 + \sqrt{53809}}{394},
$$
such that $k = f^*(\theta_\mathrm{max} \mid \bfy)$. This however is found numerically with the `optimize()`-function in `R` below. We then simulate $\Theta \sim \Uniform(0, 1)$ and $U \sim \Uniform(0, 1)$ and calculate $\alpha = f(\theta \mid \bfy) / k$. Then, if $U \leq \alpha$, $\Theta$ is returned, and if not, the procedure is run again. We then sample from the posterior distribution in the code block below.

```{r generate using rejection sampling}

y <- c(125, 18, 20, 34)   # Observed data

# Define the un-normalized posterior distribution f*(theta | y)
posterior_star <- function(theta, y) {
  return((2 + theta)^(y[1]) * (1 - theta)^(y[2] + y[3]) * theta^(y[4]))
}

# Find the normalizing constant 1 / K
norm_const <- integrate(function(theta)(posterior_star(theta, y)),
                        lower = 0,
                        upper = 1)$value

# Defining the normalized posterior distribution f(theta | y)
posterior <- function(theta, y) {
  return(posterior_star(theta, y) / norm_const)
}

# Finding the maximum 
posterior_star_max <- optimize(function(theta)(posterior_star(theta, y)),
                               interval = c(0, 1),
                               maximum = TRUE)$objective

# k such that f(theta | y) <= k
k <- posterior_star_max / norm_const

# Rejection sampling algorithm
rejection_sampling <- function(M, y) {
  count <- 0      # Count how many times the algorithm runs
  accept <- c()   # List of the accepted samples
  
  while(length(accept) < M) {   # Running until we have M accepted samples
    U <- runif(1)   # One Uniform(0, 1) sample
    Theta <- runif(1)   # One Uniform(0, 1) sample
    alpha <- posterior(Theta, y) / k
    if(U <= alpha) {
      accept <- rbind(accept, Theta)    # Appending to the list of accepted samples
    }
    count <- count + 1    # while loop has run one more time
  }
  return(list("accept" = accept, "co" = count))
}
```


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 2.
Drawing $\Theta_1, \dots, \Theta_M \sim f(\theta \mid \bfy)$, the Monte Carlo estimate of $\mu = \E(\theta \mid \bfy)$ is
$$
  \hat{\mu} = \frac{1}{M} \sum_{i=1}^M \Theta_i.
$$
We do this for $M = 10000$ in the code block below. Figure \ref{fig:Monte_Carlo} shows the result of this. We see the estimation of the posterior mean $\E(\theta \mid \bfy)$ using Monte Carlo integration and numerical integration together with the theoretical posterior density distribution and a generated histogram of the samples. In the figure the posterior density is plotted using a normalizing constant we find by numerical integration in `R`.

```{r Monte Carlo, fig.cap = "\\label{fig:Monte_Carlo}Estimation of the posterior mean $\\E(\\theta \\mid \\bfy)$ using Monte Carlo integration and numerical integration. A histogram of the samples is also shown together with the theoretical posterior density distribution."}

set.seed(69)

M <- 10000    # Number of samples from f(theta | y)

Theta_samp <- rejection_sampling(M, y)   # M samples from f(theta | y)
mu_est <- mean(Theta_samp$accept)    # = 1/M * sum(Theta_samp)

mu_num <- integrate(function(theta)(theta * posterior(theta, y)),
                    lower = 0,
                    upper = 1)$value    # Value of mu by numerical integration

# Plot
ggplot() +
  geom_histogram(
    data = as.data.frame(Theta_samp$accept),
    mapping = aes(x = Theta_samp$accept, y = ..density..),
    binwidth = 0.01,
    boundary = 0
  ) +
  stat_function(
    fun = posterior,
    args = list(y = y),
    aes(col = "Posterior density")
  ) +
  geom_vline(
    aes(xintercept = c(mu_est, mu_num),
        col = c("Estimated posterior mean", "Numerical posterior mean"),
        linetype = c("dashed", "dotted"))
  ) +
  guides(linetype = FALSE) +    # Remove linetype from label
  ggtitle("Estimation of the posterior mean") +
  xlab("theta") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.title = element_blank())
```

In the following code block we find the values of `mu_est` and `mu_num`.
```{r, eval = TRUE}
mu_est
mu_num
```
From this it is clear that the estimated posterior mean is $\hat{\mu} \approx r `mu_est`$ using Monte Carlo integration, and $\mu \approx `r mu_num`$ using numerical integration with `integrate()`. Figure \ref{fig:Monte_Carlo} also shows that these means corresponds well to the real posterior mean.


[//]: # ----------------------------------------------------------------------------------------------------------------

## Subproblem 3.
We are now interested in the number of random numbers the sampling algorithm needs to obtain one sample from $f(\theta \mid \bfy)$. The expected number of trials up to the first sample from $f(\theta \mid \bfy)$ is $c$ given by the condition
$$
  \frac{f(\theta \mid \bfy)}{g(\theta \mid \bfy)} = f(\theta \mid \bfy) \leq k.
$$
We may then choose
$$
  k \geq \max_{\theta \in [0, 1]} f(\theta \mid \bfy),
$$
and we chose the equality earlier. This was found earlier and stored in the variable `k`, which we can see the value of in the code block below.
```{r, eval = TRUE}
k
```
Using the sampler, the expected number of random numbers that has to be generated in order to obtain one sample of $f(\theta \mid \bfy)$ is the amount of times the while loop runs divided by the length amount of samples of $f(\theta \mid \bfy)$. This value is given in the following code block.
```{r using the sampling}
Theta_samp$co / M
```
These corresponds well, and we see that we need to generate approximately $7.8$ random numbers in order to obtain one sample of $f(\theta \mid \bfy)$.



## Subsection 4.

We now assume a $\Betadist(1, 5)$ prior
$$
  \tilde{f}(\theta) = \frac{1}{\mathrm{B}(1, 5)} (1 - \theta)^4 \propto (1 - \theta)^4,
$$
where $\mathrm{B}(\cdot, \cdot)$ is the beta function. This gives the posterior
$$
  \tilde{f}(\theta \mid \bfy) \propto f(\bfy \mid \theta) \tilde{f}(\theta) \propto \tilde{f}^*(\theta \mid \bfy) := (2 + \theta)^{y_1} (1 - \theta)^{y_2 + y_3 + 4} \theta^{y_4}.
$$
We wish to estimate $\mu$ by importance sampling, and to avoid needing to know the normalizing constant, we use the self-normalizing importance sampling estimator
$$
  \tilde{\mu}_\mathrm{IS} = \frac{\sum_{i=1}^n \Theta_i w(\Theta_i)}{\sum_{i=1}^n w(\Theta_i)},
$$
where
$$
  w(\Theta_i) = \frac{\tilde{f}^*(\Theta_i \mid \bfy)}{f^*(\Theta_i)} = (1 - \Theta_i)^4,
$$
and $\Theta_1, \dots, \Theta_n \sim f(\theta \mid \bfy)$. In the limit $n \to \infty$, the estimator $\tilde{\mu}_\mathrm{IS}$ is unbiased.

```{r importance sampling}
set.seed(69)

posterior_tilde_star <- function(theta, y) {
  return((2 + theta)^(y[1]) * (1 - theta)^(y[2] + y[3] + 4) * theta^(y[4]))
}

importance_sampling <- function(n, y) {
  Theta <- rejection_sampling(n, y)$accept    # Sample Theta from f(theta | y)
  w <- (1 - Theta)^4    # Calculate the weights w
  importance_mean <- sum(Theta * w) / sum(w)    # Self-normalizing importance sampling
  return(importance_mean)
}

# Test
n <- 10000
y <- c(125, 18, 20, 34)   # Observed data

mu_is <- importance_sampling(n, y)    # Estimated using importance sampling
mu_is

# Finding mu using numerical integration
const <- integrate(function(theta)(posterior_tilde_star(theta, y)),
                   lower = 0,
                   upper = 1)$value
true_mu <- integrate(function(theta)(theta * posterior_tilde_star(theta, y) / const),
                     lower = 0,
                     upper = 1)$value
true_mu

```

We then see that the estimated $\tilde{\mu}_\mathrm{IS} \approx 0.5948$, while the posterior mean using numerical integration is $\mu \approx 0.5959$, both of which corresponds well. We notice that $\tilde{\mu}_\mathrm{IS} < \hat{\mu}$, as found in Subproblem 2. This can be explained by looking at the priors. The previous prior $\Betadist(1, 1)$ and the new prior $\Betadist(1, 5)$ are shown in Figure \ref{fig:priors}. From this it is clear that the $\Betadist(1, 5)$ prior favors lower values for $\theta$ compared to the uniform prior, which do not favor any particular $\theta$. It is therefore expected that $\tilde{\mu}_\mathrm{IS} < \hat{\mu}$, which is shown to be true.

```{r priors, fig.cap = "\\label{fig:priors}The two priors $\\Betadist(1, 1)$ and $\\Betadist(1, 5)$."}
ggplot() +
  stat_function(
    fun = dbeta,
    args = list(shape1 = 1, shape2 = 1),
    aes(col = "Beta(1, 1)")
  ) +
  stat_function(
    fun = dbeta,
    args = list(shape1 = 1, shape2 = 5),
    aes(col = "Beta(1, 5)")
  ) +
  ggtitle("The different priors") +
  xlab("theta") +
  ylab("Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.title = element_blank())
```



[//]: # ----------------------------------------------------------------------------------------------------------------
[//]: # ----------------------------------------------------------------------------------------------------------------